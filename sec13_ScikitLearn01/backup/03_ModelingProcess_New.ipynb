{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "https://scikit-learn.org/stable/getting_started.html\n",
    "    \n",
    "We discuss the main features that scikit-learn provides. \n",
    "\n",
    "It assumes a very basic working knowledge of machine learning practices (model fitting, predicting, cross-validation, etc.). \n",
    "\n",
    "Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. \n",
    "\n",
    "It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting and predicting: estimator basics\n",
    "\n",
    "Scikit-learn provides dozens of built-in machine learning algorithms and models, called estimators. \n",
    "\n",
    "Each estimator can be fitted to some data using its fit method.\n",
    "\n",
    "## Example: \n",
    "\n",
    "https://scikit-learn.org/stable/getting_started.html\n",
    "\n",
    "Use RandomForestClassifier to fit training data and perform prediction\n",
    "\n",
    "- Training/Fitting:\n",
    "\n",
    "    - The fit method generally accepts 2 inputs: The sample matrix X and target value y.\n",
    "        \n",
    "        - The samples matrix (or design matrix) X. The size of X is typically (n_samples, n_features), which means that samples are represented as rows and features are represented as columns.\n",
    "        - The target values y which are real numbers for regression tasks, or integers for classification (or any other discrete set of values). For unsupervized learning tasks, y does not need to be specified. y is usually 1d array where the i th entry corresponds to the target of the i th sample (row) of X.\n",
    "    \n",
    "    - Both X and y are usually expected to be numpy arrays or equivalent array-like data types, though some estimators work with other formats such as sparse matrices.\n",
    "\n",
    "- Prediction:\n",
    "\n",
    "    - Once the estimator is fitted, it can be used for predicting target values of new data. \n",
    "    - You don't need to re-train the estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform Random Forest Classifier:\n",
      "RandomForestClassifier(random_state=0)\n",
      "\n",
      "clf.predict(X):\n",
      "[0 1]\n",
      "\n",
      "clf.predict([[1, 2, 3], [4, 5, 6], [7, 8, 9], [11, 12, 13],[14, 15, 16]]):\n",
      "[0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "X = [[ 1,  2,  3],  # 2 samples, 3 features\n",
    "     [11, 12, 13]]\n",
    "y = [0, 1]  # classes of each sample\n",
    "clf.fit(X, y)\n",
    "# take 2 samples and 3 features\n",
    "# Output is classes of each sample\n",
    "print('Perform Random Forest Classifier:')\n",
    "print(RandomForestClassifier(random_state=0))\n",
    "print()\n",
    "# Predict of training data\n",
    "print('clf.predict(X):')  # predict classes of the training data\n",
    "print(clf.predict(X))\n",
    "print()\n",
    "# array([0, 1])\n",
    "\n",
    "# predict of new data set\n",
    "print('clf.predict([[1, 2, 3], [4, 5, 6], [7, 8, 9], [11, 12, 13],[14, 15, 16]]):')  \n",
    "# predict classes of new data\n",
    "print(clf.predict([[1, 2, 3], [4, 5, 6], [7, 8, 9], [11, 12, 13],[14, 15, 16]]))\n",
    "# array([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers and pre-processors\n",
    "\n",
    "https://scikit-learn.org/stable/getting_started.html\n",
    "    \n",
    "Machine learning workflows are often composed of different parts. \n",
    "\n",
    "- A typical pipeline consists of a pre-processing step that transforms or imputes the data, and a final predictor that predicts target values.\n",
    "- In scikit-learn, pre-processors and transformers follow the same API as the estimator objects (they actually all inherit from the same BaseEstimator class). \n",
    "- The transformer objects don't have a predict method but rather a transform method that outputs a newly transformed sample matrix X:\n",
    "\n",
    "## Example: Transform by Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler().fit(X).transform(X):\n",
      "[[-1.  1.]\n",
      " [ 1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = [[0, 15],\n",
    "     [1, -10]]\n",
    "\n",
    "print('StandardScaler().fit(X).transform(X):')\n",
    "print(StandardScaler().fit(X).transform(X))\n",
    "#array([[-1.,  1.],\n",
    "#       [ 1., -1.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you want to apply different transformations to different features: the ColumnTransformer is designed for these use-cases.\n",
    "    \n",
    "# Pipelines: chaining pre-processors and estimators\n",
    "\n",
    "https://scikit-learn.org/stable/getting_started.html\n",
    "\n",
    "Transformers and estimators (predictors) can be combined together into a single unifying object: a Pipeline. The pipeline offers the same API as a regular estimator: \n",
    "it can be fitted and used for prediction with fit and predict. \n",
    "        \n",
    "As we will see later, using a pipeline will also prevent you from data leakage, \n",
    "i.e., disclosing some testing data in your training data.\n",
    "\n",
    "## Example: Load iris dataset and Pipeline Transform\n",
    "\n",
    "We load the Iris dataset, we use train_test_split() to split it into train and test sets.\n",
    "\n",
    "We then compute the accuracy score of a pipeline on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipe.fit(X_train, y_train):\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('logisticregression', LogisticRegression(random_state=0))])\n",
      "\n",
      "accuracy_score(pipe.predict(X_test), y_test):\n",
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# create a pipeline object\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression(random_state=0))\n",
    "\n",
    "# load the iris dataset and split it into train and test sets\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# fit the whole pipeline\n",
    "# 1. standard scaller \n",
    "# 2. perform logic regression.\n",
    "print('pipe.fit(X_train, y_train):')\n",
    "print(pipe.fit(X_train, y_train))\n",
    "print()\n",
    "# Pipeline(steps=[('standardscaler', StandardScaler()), \\\n",
    "#                ('logisticregression', LogisticRegression(random_state=0))])\n",
    "\n",
    "# we can now use it like any other estimator\n",
    "print('accuracy_score(pipe.predict(X_test), y_test):')\n",
    "print(accuracy_score(pipe.predict(X_test), y_test))\n",
    "# 0.97..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Cross Validation\n",
    "\n",
    "In the last example, we use train_test_split() to splits a dataset into train and test sets.\n",
    "\n",
    "The scikit-learn provides many other tools for model evaluation, in particular for cross-validation.\n",
    "\n",
    "In the below example fitting a model to automatic generate data.\n",
    "\n",
    "It does not mean that it will predict well on unseen data. \n",
    "\n",
    "We evaluated the result by the cross-validation. \n",
    "\n",
    "We use a 5-fold cross-validation procedure. \n",
    "\n",
    "Note that it is also possible to manually iterate over the folds, use different data splitting strategies, and use custom scoring functions. \n",
    "\n",
    "## Example: Model Evaluation and Cross Validation\n",
    "\n",
    "- We use make_regression to generate X, y dataset.\n",
    "\n",
    "- Then use LinearRegression() to fit the model\n",
    "\n",
    "- Then we use cross_validate function to perform the cross validation and score the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = cross_validate(lr, X, y):\n",
      "result['test_score']:\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "X, y = make_regression(n_samples=1000, random_state=0)\n",
    "lr = LinearRegression()\n",
    "\n",
    "# defaults to 5-fold Cross-validation\n",
    "print('result = cross_validate(lr, X, y):')\n",
    "result = cross_validate(lr, X, y)  \n",
    "\n",
    "# r_squared score is high because dataset is easy\n",
    "print(\"result['test_score']:\") \n",
    "print(result['test_score'])\n",
    "#array([1., 1., 1., 1., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Hyper-Parameter Searches\n",
    "\n",
    "https://scikit-learn.org/stable/getting_started.html\n",
    "    \n",
    "All estimators have parameters (often called hyper-parameters in the literature) that can be tuned. \n",
    "\n",
    "The generalization power of an estimator often critically depends on a few parameters. \n",
    "\n",
    "For example a RandomForestRegressor has a n_estimators parameter that determines the number of trees in the forest, and a max_depth parameter that determines the maximum depth of each tree. \n",
    "\n",
    "Quite often, it is not clear what the exact values of these parameters should be since they depend on the data at hand.\n",
    "\n",
    "Scikit-learn provides tools to automatically find the best parameter combinations (via cross-validation). \n",
    "\n",
    "In the following example, we randomly search over the parameter space of a random forest with a RandomizedSearchCV object. \n",
    "\n",
    "When the search is over, the RandomizedSearchCV behaves as a RandomForestRegressor that has been fitted with the best set of parameters.\n",
    "\n",
    "## Example: Automatic Hyper-Parameter Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search.fit(X_train, y_train):\n",
      "RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0), n_iter=5,\n",
      "                   param_distributions={'max_depth': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000023340C31408>,\n",
      "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000023340149AC8>},\n",
      "                   random_state=0)\n",
      "\n",
      "search.best_params_:\n",
      "{'max_depth': 9, 'n_estimators': 4}\n",
      "\n",
      "search.score(X_test, y_test):\n",
      "0.735363411343253\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import randint\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# define the parameter space that will be searched over\n",
    "param_distributions = {'n_estimators': randint(1, 5),\n",
    "                       'max_depth': randint(5, 10)}\n",
    "\n",
    "# now create a searchCV object and fit it to the data\n",
    "search = RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0), \n",
    "                            n_iter=5,\n",
    "                            param_distributions=param_distributions, \n",
    "                            random_state=0)\n",
    "\n",
    "print('search.fit(X_train, y_train):')\n",
    "print(search.fit(X_train, y_train))\n",
    "print()\n",
    "#RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0), n_iter=5,\n",
    "#                   param_distributions={'max_depth': ...,\n",
    "#                                        'n_estimators': ...},\n",
    "#                                        binrandom_state=0)\n",
    "print('search.best_params_:')\n",
    "print(search.best_params_)\n",
    "print()\n",
    "# {'max_depth': 9, 'n_estimators': 4}\n",
    "\n",
    "# the search object now acts like a normal random forest estimator\n",
    "# with max_depth=9 and n_estimators=4\n",
    "print('search.score(X_test, y_test):')\n",
    "print(search.score(X_test, y_test))\n",
    "#0.73..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "https://scikit-learn.org/stable/getting_started.html#\n",
    "    \n",
    "We discussed estimator fitting and predicting, pre-processing steps, pipelines, cross-validation tools and automatic hyper-parameter searches. \n",
    "\n",
    "This should give you an overview of some of the main features of the scikit-learn library.\n",
    "\n",
    "You can also provide the public API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
